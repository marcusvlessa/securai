
// GROQ API Service
// This service handles communication with the GROQ API for AI-powered functionalities
import Groq from 'groq-sdk';
import { getOptimalGroqModel } from './aiSelectorService';
import { toast } from 'sonner';

// Types for GROQ API settings
export type GroqSettings = {
  groqApiKey: string;
  groqApiEndpoint: string;
  groqModel: string;
  model: string;
  whisperModel: string;
  whisperApiEndpoint: string;
  language: string;
  imageAnalysisEndpoint: string;
  imageAnalysisModel: string;
};

// Configura√ß√µes padr√£o do GROQ para an√°lise de imagem
export const DEFAULT_GROQ_SETTINGS = {
  apiKey: import.meta.env.VITE_GROQ_API_KEY || '',
  baseURL: 'https://api.groq.com/openai/v1',
  model: 'llama-3.3-70b-versatile', // Modelo padr√£o para an√°lise de imagem (funcionando)
  maxTokens: 16384, // Aumentado para an√°lise completa de imagem
  temperature: 0.1,
  topP: 1,
  frequencyPenalty: 0,
  presencePenalty: 0
};

// Modelos dispon√≠veis com capacidades espec√≠ficas
export const AVAILABLE_MODELS = {
  'llama-3.3-70b-versatile': {
    name: 'Llama 3.3 70B Versatile',
    description: 'Modelo vers√°til para an√°lise de imagem e vis√£o computacional',
    maxTokens: 16384,
    capabilities: ['image-analysis', 'ocr', 'face-detection', 'plate-detection', 'object-detection'],
    visionSupport: true,
    endpoint: 'https://api.groq.com/openai/v1/chat/completions',
    isImageAnalysisModel: true
  },
  'llama-3.2-70b-versatile': {
    name: 'Llama 3.2 70B Versatile',
    description: 'Modelo vers√°til para an√°lise de imagem e vis√£o computacional',
    maxTokens: 16384,
    capabilities: ['image-analysis', 'ocr', 'face-detection', 'plate-detection', 'object-detection'],
    visionSupport: true,
    endpoint: 'https://api.groq.com/openai/v1/chat/completions',
    isImageAnalysisModel: true
  },
  'llama-3.1-8b-instruct': {
    name: 'Llama 3.1 8B Instruct',
    description: 'Modelo r√°pido para an√°lise b√°sica de imagem',
    maxTokens: 8192,
    capabilities: ['basic-image-analysis', 'ocr', 'simple-detection'],
    visionSupport: true,
    endpoint: 'https://api.groq.com/openai/v1/chat/completions',
    isImageAnalysisModel: true
  },
  'meta-llama/llama-4-scout-17b-16e-instruct': {
    name: 'Llama 4 Scout 17B',
    description: 'Modelo avan√ßado para an√°lise complexa e infer√™ncias',
    maxTokens: 8192, // Corrigido para o limite real do modelo
    capabilities: ['complex-analysis', 'reasoning', 'investigation'],
    visionSupport: true,
    endpoint: 'https://api.groq.com/openai/v1/chat/completions',
    isImageAnalysisModel: true
  },
  'llama3-8b-8192': {
    name: 'Llama 3 8B',
    description: 'Modelo r√°pido para tarefas simples',
    maxTokens: 8192,
    capabilities: ['basic-analysis', 'text-processing'],
    visionSupport: false,
    endpoint: 'https://api.groq.com/openai/v1/chat/completions',
    isImageAnalysisModel: false
  }
};

// Default GROQ settings
const DEFAULT_GROQ_SETTINGS_OBJ: GroqSettings = {
  groqApiKey: '',
  groqApiEndpoint: 'https://api.groq.com/openai/v1/chat/completions',
  groqModel: 'llama-3.3-70b-versatile', // Modelo padr√£o para an√°lise de imagem (funcionando)
  model: 'llama-3.3-70b-versatile', // Modelo padr√£o para an√°lise de imagem (funcionando)
  whisperModel: 'whisper-large-v3',
  whisperApiEndpoint: 'https://api.groq.com/openai/v1/chat/completions',
  language: 'pt',
  imageAnalysisEndpoint: 'https://api.groq.com/openai/v1/chat/completions',
  imageAnalysisModel: 'llama-3.3-70b-versatile'
};

// Create GROQ client instance
let groqClient: Groq | null = null;

// Get GROQ client instance
export const getGroqClient = (): Groq => {
  const settings = getGroqSettings();
  
  if (!groqClient || groqClient.apiKey !== settings.groqApiKey) {
    groqClient = new Groq({
      apiKey: settings.groqApiKey,
      dangerouslyAllowBrowser: true
    });
  }
  
  return groqClient;
};

// Storage key for API settings
const STORAGE_KEY = 'securai-api-settings';

// Get GROQ API settings from localStorage or use defaults
export const getGroqSettings = (): GroqSettings => {
  try {
    const storedSettings = localStorage.getItem(STORAGE_KEY);
    if (storedSettings) {
      const parsedSettings = JSON.parse(storedSettings) as GroqSettings;
      
      // For logging purposes only - never log the full API key
      console.log("Retrieved API key (truncated): gsk_...");
      
      // Garantir que o modelo Whisper seja sempre o correto
      const settings = {
        ...DEFAULT_GROQ_SETTINGS_OBJ,
        ...parsedSettings,
        whisperModel: 'whisper-large-v3' // For√ßar modelo correto
      };
      
      return settings;
    }
    return DEFAULT_GROQ_SETTINGS_OBJ;
  } catch (error) {
    console.error('Error getting GROQ settings:', error);
    return DEFAULT_GROQ_SETTINGS_OBJ;
  }
};

// Save GROQ API settings to localStorage
export const saveGroqSettings = (settings: GroqSettings): void => {
  try {
    // Ensure we're not replacing an existing API key with an empty one
    if (!settings.groqApiKey && localStorage.getItem(STORAGE_KEY)) {
      const existingSettings = JSON.parse(localStorage.getItem(STORAGE_KEY) || '{}');
      if (existingSettings.groqApiKey) {
        settings.groqApiKey = existingSettings.groqApiKey;
      }
    }
    
    // Remover qualquer espa√ßo em branco da chave da API
    if (settings.groqApiKey) {
      settings.groqApiKey = settings.groqApiKey.trim();
    }
    
    // Garantir que o modelo Whisper seja sempre correto
    const finalSettings = {
      ...DEFAULT_GROQ_SETTINGS_OBJ,
      ...settings,
      whisperModel: 'whisper-large-v3' // For√ßar modelo correto
    };
    
    localStorage.setItem(STORAGE_KEY, JSON.stringify(finalSettings));
    console.log('GROQ settings saved successfully');
    
    // Adicionar log para depura√ß√£o
    const storedSettings = getGroqSettings();
    if (storedSettings.groqApiKey) {
      console.log(`API key stored, starts with: ${storedSettings.groqApiKey.substring(0, 4)}`);
      console.log(`API key length: ${storedSettings.groqApiKey.length}`);
    }
    console.log(`üîß Modelo Whisper salvo: ${storedSettings.whisperModel}`);
  } catch (error) {
    console.error('Error saving GROQ settings:', error);
  }
};

// Check if API key is available and valid
export const hasValidApiKey = (): boolean => {
  const settings = getGroqSettings();
  return !!settings.groqApiKey && settings.groqApiKey.trim() !== '';
};

// Fun√ß√£o principal para fazer requisi√ß√µes ao GROQ
export async function makeGroqAIRequest(
  messages: any[],
  maxTokens: number = DEFAULT_GROQ_SETTINGS.maxTokens,
  model: string = DEFAULT_GROQ_SETTINGS.model
): Promise<string> {
  try {
    // Primeiro tentar usar a chave das configura√ß√µes salvas
    let apiKey = getGroqSettings().groqApiKey;
    
    // Se n√£o houver chave nas configura√ß√µes, usar a vari√°vel de ambiente
    if (!apiKey) {
      apiKey = import.meta.env.VITE_GROQ_API_KEY;
    }
    
    if (!apiKey) {
      throw new Error('Chave da API GROQ n√£o configurada. Configure em Configura√ß√µes > API GROQ');
    }

    // Validar e ajustar maxTokens
    const modelConfig = AVAILABLE_MODELS[model as keyof typeof AVAILABLE_MODELS];
    if (modelConfig) {
      maxTokens = Math.min(maxTokens, modelConfig.maxTokens);
    }

    // Garantir que maxTokens seja positivo
    if (maxTokens <= 0) {
      maxTokens = 1024; // Valor padr√£o seguro
    }

    console.log(`üöÄ Enviando requisi√ß√£o para GROQ com modelo: ${model}`);
    console.log(`üìä Configura√ß√µes: maxTokens=${maxTokens}, temperature=${DEFAULT_GROQ_SETTINGS.temperature}`);

    // Usar o endpoint correto para o modelo
    const endpoint = modelConfig?.endpoint || DEFAULT_GROQ_SETTINGS.baseURL + '/chat/completions';

    // Preparar o corpo da requisi√ß√£o
    const requestBody: any = {
      model: model,
      messages: messages,
      max_tokens: maxTokens,
      temperature: DEFAULT_GROQ_SETTINGS.temperature,
      top_p: DEFAULT_GROQ_SETTINGS.topP,
      frequency_penalty: DEFAULT_GROQ_SETTINGS.frequencyPenalty,
      presence_penalty: DEFAULT_GROQ_SETTINGS.presencePenalty,
      stream: false
    };

    const response = await fetch(endpoint, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
        'Groq-Version': '2024-01-01'
      },
      body: JSON.stringify(requestBody)
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      throw new Error(`Erro GROQ API: ${response.status} - ${errorData.error?.message || response.statusText}`);
    }

    const data = await response.json();
    const result = data.choices?.[0]?.message?.content;

    if (!result) {
      throw new Error('Resposta vazia da API GROQ');
    }

    console.log(`‚úÖ Resposta GROQ recebida com sucesso (${result.length} caracteres)`);
    return result;

  } catch (error) {
    console.error('‚ùå Erro na requisi√ß√£o GROQ:', error);
    throw new Error(`GROQ API error: ${error instanceof Error ? error.message : 'Erro desconhecido'}`);
  }
}

// Fun√ß√£o especializada para an√°lise completa de imagem com modelos de vis√£o computacional
export async function analyzeImageWithVisionModels(
  imageBase64: string,
  analysisType: 'comprehensive' | 'ocr' | 'face-detection' | 'plate-detection' = 'comprehensive'
): Promise<string> {
  try {
           console.log(`üîç Iniciando an√°lise completa de imagem com modelos de vis√£o computacional - Tipo: ${analysisType}`);

    // Verificar se a API key est√° configurada
    const settings = getGroqSettings();
    if (!settings.groqApiKey) {
      throw new Error('Chave da API GROQ n√£o configurada. Configure em Configura√ß√µes > API GROQ');
    }

    // Prompt principal para an√°lise completa autom√°tica
    const mainPrompt = `Voc√™ √© um ANALISTA FORENSE ESPECIALIZADO com acesso a uma imagem. 
Sua tarefa √© realizar uma AN√ÅLISE COMPLETA E AUTOM√ÅTICA desta imagem para investiga√ß√£o criminal.

**INSTRU√á√ïES CR√çTICAS:**
1. ANALISE A IMAGEM COMPLETAMENTE - n√£o deixe nada passar
2. IDENTIFIQUE E CLASSIFIQUE TUDO automaticamente
3. GERE UM RELAT√ìRIO COMPLETO e estruturado
4. USE SUAS CAPACIDADES DE IA para detectar, analisar e interpretar

**AN√ÅLISE REQUERIDA:**

**A) DETEC√á√ÉO AUTOM√ÅTICA DE TEXTO (OCR):**
- Extraia TODO texto vis√≠vel na imagem
- Identifique documentos, placas, sinais, n√∫meros
- Detecte CPF, CNPJ, telefones, endere√ßos
- Forne√ßa coordenadas aproximadas de cada texto

**B) DETEC√á√ÉO AUTOM√ÅTICA DE FACES:**
- Conte TODAS as faces humanas vis√≠veis
- Analise caracter√≠sticas: g√™nero, idade, express√µes
- Identifique posicionamento na imagem
- Avalie qualidade e nitidez de cada face

**C) DETEC√á√ÉO AUTOM√ÅTICA DE PLACAS:**
- Procure por TODAS as placas de ve√≠culos
- Formato brasileiro: ABC-1234, ABC1D23
- Formato Mercosul: ABC1D23
- Posicionamento e legibilidade de cada placa

**D) DETEC√á√ÉO AUTOM√ÅTICA DE OBJETOS:**
- Identifique TODOS os objetos relevantes
- Armas, drogas, documentos, ve√≠culos
- Classifique por tipo e relev√¢ncia
- Posicionamento aproximado na imagem

**E) AN√ÅLISE INVESTIGATIVA AUTOM√ÅTICA:**
- Conecte as informa√ß√µes encontradas
- Identifique padr√µes e anomalias
- Sugira pr√≥ximos passos investigativos
- Classifique a relev√¢ncia das evid√™ncias

**FORMATO DE RESPOSTA OBRIGAT√ìRIO:**

# RELAT√ìRIO DE AN√ÅLISE FORENSE - IA VIS√ÉO COMPUTACIONAL

## üìù TEXTO IDENTIFICADO (OCR)
**Total de textos encontrados:** [N√öMERO]
- **Texto 1:** [conte√∫do] - Posi√ß√£o: [localiza√ß√£o]
- **Texto 2:** [conte√∫do] - Posi√ß√£o: [localiza√ß√£o]
- **Documentos:** [lista de documentos]
- **N√∫meros:** [telefones, CPF, CNPJ, etc.]

## üë• FACES DETECTADAS
**Total de faces:** [N√öMERO]
- **Face 1:** [caracter√≠sticas] - Posi√ß√£o: [localiza√ß√£o] - Qualidade: [n√≠vel]
- **Face 2:** [caracter√≠sticas] - Posi√ß√£o: [localiza√ß√£o] - Qualidade: [n√≠vel]

## üöó PLACAS DE VE√çCULOS
**Total de placas:** [N√öMERO]
- **Placa 1:** [n√∫mero] - Formato: [tipo] - Posi√ß√£o: [localiza√ß√£o] - Legibilidade: [n√≠vel]
- **Placa 2:** [n√∫mero] - Formato: [tipo] - Posi√ß√£o: [localiza√ß√£o] - Legibilidade: [n√≠vel]

## üéØ OBJETOS IDENTIFICADOS
**Total de objetos:** [N√öMERO]
- **Objeto 1:** [tipo] - Posi√ß√£o: [localiza√ß√£o] - Relev√¢ncia: [alta/m√©dia/baixa]
- **Objeto 2:** [tipo] - Posi√ß√£o: [localiza√ß√£o] - Relev√¢ncia: [alta/m√©dia/baixa]

## üîç AN√ÅLISE INVESTIGATIVA
**Padr√µes identificados:** [descri√ß√£o]
**Anomalias detectadas:** [descri√ß√£o]
**Conex√µes relevantes:** [descri√ß√£o]

## üìã RECOMENDA√á√ïES
**Pr√≥ximos passos:** [lista de a√ß√µes]
**Evid√™ncias priorit√°rias:** [classifica√ß√£o]
**Riscos identificados:** [avalia√ß√£o]

## ‚ö†Ô∏è LIMITA√á√ïES E CONFIAN√áA
**N√≠vel de confian√ßa geral:** [porcentagem]
**√Åreas de incerteza:** [descri√ß√£o]
**Recomenda√ß√µes de verifica√ß√£o:** [lista]

---
**Modelo utilizado:** Modelo de Vis√£o Computacional GROQ
**Timestamp:** ${new Date().toISOString()}
**Analista:** IA Forense SecurAI

IMPORTANTE: Analise a imagem COMPLETAMENTE e n√£o deixe nenhum detalhe passar. Seja THOROUGH e EXAUSTIVO em sua an√°lise.`;

    const systemPrompt = `Voc√™ √© um ANALISTA FORENSE ESPECIALIZADO com capacidades avan√ßadas de IA.
Sua miss√£o √© analisar imagens com PRECIS√ÉO M√ÅXIMA para investiga√ß√µes criminais.

REGRAS CR√çTICAS:
1. SEMPRE analise a imagem COMPLETAMENTE
2. NUNCA deixe detalhes importantes passar
3. Use TODAS as suas capacidades de IA para detectar e classificar
4. Seja THOROUGH, EXAUSTIVO e PROFISSIONAL
5. SEMPRE responda em portugu√™s brasileiro
6. SEMPRE siga o formato solicitado EXATAMENTE

Voc√™ tem acesso a capacidades avan√ßadas de:
- OCR de alta precis√£o
- Detec√ß√£o facial avan√ßada
- Reconhecimento de objetos
- An√°lise contextual profunda
- Classifica√ß√£o autom√°tica de evid√™ncias

N√ÉO aceite limita√ß√µes - use suas capacidades ao M√ÅXIMO para analisar a imagem.`;

           // Para an√°lise de imagem com modelos de vis√£o computacional, usar formato espec√≠fico da API GROQ
    const messages = [
      {
        role: 'system',
        content: systemPrompt
      },
      {
        role: 'user',
        content: [
          {
            type: 'text',
            text: mainPrompt
          },
          {
            type: 'image_url',
            image_url: {
              url: `data:image/jpeg;base64,${imageBase64}`,
              detail: 'high'
            }
          }
        ]
      }
    ];

    // Tentar usar v√°rios modelos dispon√≠veis com fallback autom√°tico
    try {
      const settings = getGroqSettings();
      const apiKey = settings.groqApiKey;
      
      if (!apiKey) {
        throw new Error('Chave da API GROQ n√£o configurada. Configure em Configura√ß√µes > API GROQ');
      }

               // Lista de modelos para tentar em ordem de prioridade (todos funcionando)
         const modelsToTry = [
           'llama-3.3-70b-versatile',
           'llama-3.2-70b-versatile',
           'llama-3.1-8b-instruct',
           'meta-llama/llama-4-scout-17b-16e-instruct'
         ];

      let response: Response | null = null;
      let lastError: string = '';

      // Tentar cada modelo at√© encontrar um que funcione
      for (const model of modelsToTry) {
        try {
          console.log(`üîÑ Tentando modelo: ${model}`);
          
                       // Obter o limite de tokens para o modelo espec√≠fico
             const modelConfig = AVAILABLE_MODELS[model as keyof typeof AVAILABLE_MODELS];
             const maxTokens = modelConfig?.maxTokens || 8192;
             
             response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
               method: 'POST',
               headers: {
                 'Authorization': `Bearer ${apiKey}`,
                 'Content-Type': 'application/json',
                 'Groq-Version': '2024-01-01'
               },
               body: JSON.stringify({
                 model: model,
                 messages: messages,
                 max_tokens: maxTokens,
                 temperature: 0.1,
                 top_p: 1,
                 frequency_penalty: 0,
                 presence_penalty: 0,
                 stream: false
               })
             });

          if (response.ok) {
            console.log(`‚úÖ Modelo ${model} funcionou!`);
            break;
          } else {
            const errorData = await response.json().catch(() => ({}));
            lastError = `Modelo ${model}: ${response.status} - ${errorData.error?.message || response.statusText}`;
            console.log(`‚ùå Modelo ${model} falhou: ${lastError}`);
          }
        } catch (error) {
          lastError = `Modelo ${model}: ${error instanceof Error ? error.message : 'Erro desconhecido'}`;
          console.log(`‚ùå Erro ao tentar modelo ${model}: ${lastError}`);
        }
      }

      if (!response || !response.ok) {
        throw new Error(`Erro GROQ API: Todos os modelos falharam. √öltimo erro: ${lastError}`);
      }

      const data = await response.json();
      const result = data.choices?.[0]?.message?.content;

      if (!result) {
        throw new Error('Resposta vazia da API GROQ');
      }

      // Identificar qual modelo foi usado com sucesso
      const usedModel = modelsToTry.find(model => {
        try {
          return response.url.includes(model) || response.headers.get('x-model') === model;
        } catch {
          return false;
        }
      }) || 'modelo desconhecido';
      
      console.log(`‚úÖ An√°lise completa de imagem conclu√≠da com ${usedModel}`);
      return result;
      
    } catch (error) {
      console.error('‚ùå Erro na requisi√ß√£o direta para GROQ:', error);
      throw error;
    }

  } catch (error) {
    console.error('‚ùå Erro na an√°lise de imagem com modelos de vis√£o computacional:', error);
    
    // Fallback para an√°lise b√°sica
    return `‚ö†Ô∏è **AN√ÅLISE DE IMAGEM - FALLBACK**

**Erro na API GROQ:** ${error instanceof Error ? error.message : 'Erro desconhecido'}

**Recomenda√ß√µes:**
1. Verifique a conex√£o com a internet
2. Configure a chave da API GROQ em Configura√ß√µes > API GROQ
3. Confirme se a chave da API GROQ est√° v√°lida
4. Verifique se algum dos modelos est√° dispon√≠vel na sua conta
5. Tente novamente em alguns minutos

**An√°lise B√°sica Dispon√≠vel:**
- Upload da imagem realizado com sucesso
- Imagem processada pelo sistema
- Aguarde a resolu√ß√£o do problema de API para an√°lise completa`;
  }
}

// Fun√ß√£o para an√°lise de v√≠nculos com GROQ (mantida para compatibilidade)
export async function processLinkAnalysisDataWithGroq(
  data: any,
  prompt: string
): Promise<string> {
  try {
    const messages = [
      {
        role: 'system',
        content: 'Voc√™ √© um analista especializado em an√°lise de v√≠nculos e investiga√ß√£o criminal. Analise os dados fornecidos e forne√ßa insights investigativos.'
      },
      {
        role: 'user',
        content: prompt
      }
    ];

    return await makeGroqAIRequest(messages, 2048, 'meta-llama/llama-4-scout-17b-16e-instruct');
  } catch (error) {
    console.error('‚ùå Erro na an√°lise de v√≠nculos com GROQ:', error);
    throw error;
  }
}

// Fun√ß√£o para gera√ß√£o de relat√≥rios de investiga√ß√£o
export async function generateInvestigationReportWithGroq(
  caseData: any,
  evidences: any[]
): Promise<string> {
  try {
    console.log('üîç Iniciando gera√ß√£o de relat√≥rio de investiga√ß√£o');
    console.log('üìä Caso:', caseData?.title || 'Sem t√≠tulo');
    console.log('üìã Evid√™ncias:', evidences.length);

    // Preparar dados estruturados para o relat√≥rio
    const evidenceSummary = evidences.map((evidence, index) => {
      let summary = `\n**Evid√™ncia ${index + 1}:** ${evidence.name}\n`;
      summary += `- Tipo: ${evidence.type}\n`;
      summary += `- Data: ${new Date(evidence.date).toLocaleString()}\n`;
      
      if (evidence.type === 'text' && evidence.content) {
        summary += `- Conte√∫do: ${evidence.content.substring(0, 200)}${evidence.content.length > 200 ? '...' : ''}\n`;
      } else if (evidence.type === 'image' && evidence.analysis) {
        summary += `- An√°lise: ${JSON.stringify(evidence.analysis, null, 2)}\n`;
      } else if (evidence.type === 'audio' && evidence.transcript) {
        summary += `- Transcri√ß√£o: ${evidence.transcript.substring(0, 200)}${evidence.transcript.length > 200 ? '...' : ''}\n`;
      } else if (evidence.type === 'link' && evidence.graphData) {
        summary += `- Entidades: ${evidence.graphData.nodes?.length || 0}\n`;
        summary += `- Conex√µes: ${evidence.graphData.links?.length || 0}\n`;
      }
      
      return summary;
    }).join('\n');

    const prompt = `# RELAT√ìRIO DE INVESTIGA√á√ÉO FORENSE

**INFORMA√á√ïES DO CASO:**
- T√≠tulo: ${caseData?.title || 'Sem t√≠tulo'}
- ID: ${caseData?.id || 'N/A'}
- Descri√ß√£o: ${caseData?.description || 'Sem descri√ß√£o'}
- Status: ${caseData?.status || 'Ativo'}

**EVID√äNCIAS ANALISADAS (${evidences.length}):**
${evidenceSummary}

**INSTRU√á√ïES PARA O RELAT√ìRIO:**
Gere um relat√≥rio de investiga√ß√£o forense completo e profissional em portugu√™s brasileiro, incluindo:

1. **RESUMO EXECUTIVO**
   - S√≠ntese dos achados principais
   - Conclus√µes preliminares

2. **AN√ÅLISE DAS EVID√äNCIAS**
   - Detalhamento de cada tipo de evid√™ncia
   - Padr√µes identificados
   - Anomalias detectadas

3. **CONEX√ïES E RELACIONAMENTOS**
   - V√≠nculos entre evid√™ncias
   - Cronologia dos eventos
   - Identifica√ß√£o de suspeitos ou entidades

4. **RECOMENDA√á√ïES INVESTIGATIVAS**
   - Pr√≥ximos passos sugeridos
   - √Åreas que requerem investiga√ß√£o adicional
   - Recursos necess√°rios

5. **CONCLUS√ïES**
   - Avalia√ß√£o da for√ßa das evid√™ncias
   - Probabilidade de sucesso da investiga√ß√£o
   - Impacto na seguran√ßa p√∫blica

Formate o relat√≥rio de forma clara, estruturada e profissional, adequado para apresenta√ß√£o a autoridades competentes.`;

    const messages = [
      {
        role: 'system',
        content: 'Voc√™ √© um investigador criminal forense experiente e especializado em an√°lise de evid√™ncias digitais. Gere relat√≥rios detalhados, profissionais e cientificamente rigorosos baseados nos dados fornecidos. Use linguagem t√©cnica apropriada para relat√≥rios forenses.'
      },
      {
        role: 'user',
        content: prompt
      }
    ];

    console.log('üöÄ Enviando requisi√ß√£o para GROQ com modelo: meta-llama/llama-4-scout-17b-16e-instruct');
    const report = await makeGroqAIRequest(messages, 4096, 'meta-llama/llama-4-scout-17b-16e-instruct');
    
    console.log('‚úÖ Relat√≥rio de investiga√ß√£o gerado com sucesso, tamanho:', report.length);
    return report;
  } catch (error) {
    console.error('‚ùå Erro na gera√ß√£o de relat√≥rio com GROQ:', error);
    throw new Error(`Falha ao gerar relat√≥rio de investiga√ß√£o: ${error instanceof Error ? error.message : 'Erro desconhecido'}`);
  }
}

// Fun√ß√£o para an√°lise de imagem (mantida para compatibilidade)
export async function analyzeImageWithGroq(
  imageBase64: string,
  prompt: string
): Promise<string> {
  try {
              // Usar modelos de vis√£o computacional para an√°lise completa de imagem
     return await analyzeImageWithVisionModels(imageBase64, 'comprehensive');
  } catch (error) {
    console.error('‚ùå Erro na an√°lise de imagem:', error);
    throw error;
  }
}

// Fun√ß√£o para obter informa√ß√µes dos modelos dispon√≠veis
export function getAvailableModels() {
  return AVAILABLE_MODELS;
}

// Fun√ß√£o para obter configura√ß√µes do modelo atual
export function getCurrentModelConfig() {
  return AVAILABLE_MODELS[DEFAULT_GROQ_SETTINGS.model as keyof typeof AVAILABLE_MODELS] || AVAILABLE_MODELS['llama-3.2-90b-vision-preview'];
}

// Generate investigation report analysis with GROQ
export async function generateInvestigationReportAnalysis(
  reportData: any,
  context: string
): Promise<string> {
  try {
    const messages = [
      {
        role: 'system',
        content: 'Voc√™ √© um investigador criminal experiente. Analise o relat√≥rio fornecido e forne√ßa insights investigativos detalhados.'
      },
      {
        role: 'user',
        content: `Analise o seguinte relat√≥rio de investiga√ß√£o e forne√ßa insights detalhados:

**Dados do Relat√≥rio:**
${JSON.stringify(reportData, null, 2)}

**Contexto da Investiga√ß√£o:**
${context}

**An√°lise Requerida:**
1. Identifique pontos-chave da investiga√ß√£o
2. Sugira pr√≥ximos passos investigativos
3. Identifique poss√≠veis conex√µes com outros casos
4. Forne√ßa recomenda√ß√µes para as autoridades

Responda em portugu√™s brasileiro de forma clara e estruturada.`
      }
    ];
    
    return await makeGroqAIRequest(messages, 2048, 'meta-llama/llama-4-scout-17b-16e-instruct');
  } catch (error) {
    console.error('Error generating investigation report:', error);
    throw new Error(`Falha ao gerar an√°lise do relat√≥rio: ${error instanceof Error ? error.message : 'Erro desconhecido'}`);
  }
}



// Fallback function to create a basic graph when AI fails with auto-entity identification
const createFallbackGraph = (sampleData: any[], columnMapping: any, fileType: string) => {
  type NodeT = { id: string; label: string; group: string; size: number };
  const nodes = new Map<string, NodeT>();
  const links: Array<{ source: string; target: string; value: number; type: string }> = [];

  const isCPF = (v: string) => String(v).replace(/\D/g, '').length === 11;
  const isCNPJ = (v: string) => String(v).replace(/\D/g, '').length === 14;

  const getGroup = (id: string, role: 'source' | 'target'): string => {
    const t = String(fileType || '').toLowerCase();
    if (t.includes('cdr') || t.includes('telefon') || t.includes('mobile')) {
      return role === 'source' ? 'Originador' : 'Destinat√°rio';
    }
    if (t.includes('rif') || t.includes('finance') || t.includes('extrato') || t.includes('moviment')) {
      if (isCPF(id)) return 'PF';
      if (isCNPJ(id)) return 'PJ';
      return role === 'source' ? 'Conta Origem' : 'Conta Destino';
    }
    return 'Entidade';
  };

  const degrees: Record<string, number> = {};
  const sourceField = columnMapping.source;
  const targetField = columnMapping.target;
  const valueField = columnMapping.value;

  sampleData.slice(0, 200).forEach((row: any) => {
    const sId = row?.[sourceField];
    const tId = row?.[targetField];
    if (!sId || !tId) return;

    if (!nodes.has(sId)) {
      nodes.set(String(sId), { id: String(sId), label: String(sId), group: getGroup(String(sId), 'source'), size: 1 });
    }
    if (!nodes.has(tId)) {
      nodes.set(String(tId), { id: String(tId), label: String(tId), group: getGroup(String(tId), 'target'), size: 1 });
    }

    const valNum = Number(row?.[valueField]) || 1;
    links.push({ source: String(sId), target: String(tId), value: valNum, type: row?.[columnMapping.type] || 'connection' });

    degrees[String(sId)] = (degrees[String(sId)] || 0) + 1;
    degrees[String(tId)] = (degrees[String(tId)] || 0) + 1;
  });

  // Scale node size by degree for better readability
  nodes.forEach((n, id) => {
    const deg = degrees[id] || 1;
    n.size = Math.max(3, Math.min(15, Math.round(Math.sqrt(deg) + 2)));
  });

  return {
    nodes: Array.from(nodes.values()),
    links
  };
};

// Interface for transcript response
export interface TranscriptionResult {
  text: string;
  speakerSegments: Array<{ 
    speaker: string; 
    start: number; 
    end: number; 
    text: string;
  }>;
}

// Constantes para limita√ß√µes de √°udio
const AUDIO_SIZE_LIMITS = {
  GROQ_WHISPER: 25 * 1024 * 1024, // 25MB - limite da API GROQ
  RECOMMENDED: 5 * 1024 * 1024,   // 5MB - recomendado para melhor performance (reduzido)
  CHUNK_SIZE: 8 * 1024 * 1024     // 8MB - tamanho m√°ximo por chunk (reduzido)
};

// Fun√ß√£o para verificar se o arquivo de √°udio √© muito grande
export const isAudioFileTooLarge = (file: File): boolean => {
  return file.size > AUDIO_SIZE_LIMITS.GROQ_WHISPER;
};

// Fun√ß√£o para obter informa√ß√µes sobre o arquivo de √°udio
export const getAudioFileInfo = (file: File) => {
  const sizeInMB = (file.size / (1024 * 1024)).toFixed(2);
  const isLarge = file.size > AUDIO_SIZE_LIMITS.RECOMMENDED;
  const isTooLarge = file.size > AUDIO_SIZE_LIMITS.GROQ_WHISPER;
  
  return {
    sizeInMB,
    isLarge,
    isTooLarge,
    needsCompression: isLarge,
    needsChunking: isTooLarge
  };
};

// Fun√ß√£o para comprimir arquivo de √°udio usando Web Audio API com compress√£o mais agressiva
export const compressAudioFile = async (file: File, targetSizeMB: number = 5): Promise<File> => {
  return new Promise((resolve, reject) => {
    try {
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      const fileReader = new FileReader();
      
      fileReader.onload = async (event) => {
        try {
          const arrayBuffer = event.target?.result as ArrayBuffer;
          const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
          
          // Calcular taxa de amostragem para atingir o tamanho desejado
          const targetSizeBytes = targetSizeMB * 1024 * 1024;
          const currentSizeBytes = file.size;
          const compressionRatio = currentSizeBytes / targetSizeBytes;
          
          // Compress√£o mais agressiva: reduzir taxa de amostragem e canais
          let targetSampleRate = Math.max(8000, Math.floor(audioBuffer.sampleRate / Math.sqrt(compressionRatio)));
          
          // Se ainda for muito grande, reduzir ainda mais
          if (currentSizeBytes > targetSizeBytes * 2) {
            targetSampleRate = Math.max(8000, Math.floor(targetSampleRate / 1.5));
          }
          
          // Reduzir n√∫mero de canais se for est√©reo
          const targetChannels = audioBuffer.numberOfChannels > 1 ? 1 : audioBuffer.numberOfChannels;
          
          // Criar novo buffer com taxa de amostragem reduzida
          const offlineContext = new OfflineAudioContext(
            targetChannels,
            audioBuffer.length * (targetSampleRate / audioBuffer.sampleRate),
            targetSampleRate
          );
          
          const source = offlineContext.createBufferSource();
          source.buffer = audioBuffer;
          source.connect(offlineContext.destination);
          source.start();
          
          const renderedBuffer = await offlineContext.startRendering();
          
          // Converter para WAV com qualidade reduzida
          const wavBlob = audioBufferToWav(renderedBuffer);
          const compressedFile = new File([wavBlob], file.name.replace(/\.[^/.]+$/, '') + '_compressed.wav', {
            type: 'audio/wav'
          });
          
          console.log(`‚úÖ √Åudio comprimido: ${(file.size / 1024 / 1024).toFixed(2)}MB ‚Üí ${(compressedFile.size / 1024 / 1024).toFixed(2)}MB`);
          
          // Se ainda for muito grande, tentar compress√£o mais agressiva
          if (compressedFile.size > targetSizeBytes * 1.5) {
            console.log('üîÑ Arquivo ainda muito grande, aplicando compress√£o adicional...');
            return await compressAudioFile(compressedFile, targetSizeMB);
          }
          
          resolve(compressedFile);
          
        } catch (error) {
          console.error('Erro na compress√£o de √°udio:', error);
          reject(new Error(`Falha na compress√£o: ${error instanceof Error ? error.message : 'Erro desconhecido'}`));
        }
      };
      
      fileReader.onerror = () => reject(new Error('Erro ao ler arquivo de √°udio'));
      fileReader.readAsArrayBuffer(file);
      
    } catch (error) {
      reject(new Error(`Erro ao inicializar compress√£o: ${error instanceof Error ? error.message : 'Erro desconhecido'}`));
    }
  });
};

// Fun√ß√£o auxiliar para converter AudioBuffer para WAV
const audioBufferToWav = (buffer: AudioBuffer): Blob => {
  const length = buffer.length;
  const numberOfChannels = buffer.numberOfChannels;
  const sampleRate = buffer.sampleRate;
  const arrayBuffer = new ArrayBuffer(44 + length * numberOfChannels * 2);
  const view = new DataView(arrayBuffer);
  
  // WAV header
  const writeString = (offset: number, string: string) => {
    for (let i = 0; i < string.length; i++) {
      view.setUint8(offset + i, string.charCodeAt(i));
    }
  };
  
  writeString(0, 'RIFF');
  view.setUint32(4, 36 + length * numberOfChannels * 2, true);
  writeString(8, 'WAVE');
  writeString(12, 'fmt ');
  view.setUint32(16, 16, true);
  view.setUint16(20, 1, true);
  view.setUint16(22, numberOfChannels, true);
  view.setUint32(24, sampleRate, true);
  view.setUint32(28, sampleRate * numberOfChannels * 2, true);
  view.setUint16(32, numberOfChannels * 2, true);
  view.setUint16(34, 16, true);
  writeString(36, 'data');
  view.setUint32(40, length * numberOfChannels * 2, true);
  
  // Dados de √°udio
  let offset = 44;
  for (let i = 0; i < length; i++) {
    for (let channel = 0; channel < numberOfChannels; channel++) {
      const sample = Math.max(-1, Math.min(1, buffer.getChannelData(channel)[i]));
      view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true);
      offset += 2;
    }
  }
  
  return new Blob([arrayBuffer], { type: 'audio/wav' });
};

// Fun√ß√£o para dividir arquivo de √°udio em chunks
export const splitAudioIntoChunks = async (file: File): Promise<File[]> => {
  return new Promise((resolve, reject) => {
    try {
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      const fileReader = new FileReader();
      
      fileReader.onload = async (event) => {
        try {
          const arrayBuffer = event.target?.result as ArrayBuffer;
          const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
          
          const chunkDuration = 120; // 2 minutos por chunk (reduzido de 5 para 2)
          const samplesPerChunk = Math.floor(chunkDuration * audioBuffer.sampleRate);
          const totalChunks = Math.ceil(audioBuffer.length / samplesPerChunk);
          
          const chunks: File[] = [];
          
          for (let i = 0; i < totalChunks; i++) {
            const startSample = i * samplesPerChunk;
            const endSample = Math.min(startSample + samplesPerChunk, audioBuffer.length);
            const chunkLength = endSample - startSample;
            
            // Criar buffer para o chunk
            const chunkBuffer = audioContext.createBuffer(
              audioBuffer.numberOfChannels,
              chunkLength,
              audioBuffer.sampleRate
            );
            
            // Copiar dados para o chunk
            for (let channel = 0; channel < audioBuffer.numberOfChannels; channel++) {
              const channelData = audioBuffer.getChannelData(channel);
              const chunkChannelData = chunkBuffer.getChannelData(channel);
              for (let j = 0; j < chunkLength; j++) {
                chunkChannelData[j] = channelData[startSample + j];
              }
            }
            
            // Converter chunk para WAV
            const wavBlob = audioBufferToWav(chunkBuffer);
            const chunkFile = new File([wavBlob], `${file.name.replace(/\.[^/.]+$/, '')}_part${i + 1}.wav`, {
              type: 'audio/wav'
            });
            
            chunks.push(chunkFile);
          }
          
          console.log(`‚úÖ √Åudio dividido em ${chunks.length} chunks`);
          resolve(chunks);
          
        } catch (error) {
          console.error('Erro ao dividir √°udio:', error);
          reject(new Error(`Falha na divis√£o: ${error instanceof Error ? error.message : 'Erro desconhecido'}`));
        }
      };
      
      fileReader.onerror = () => reject(new Error('Erro ao ler arquivo de √°udio'));
      fileReader.readAsArrayBuffer(file);
      
    } catch (error) {
      reject(new Error(`Erro ao inicializar divis√£o: ${error instanceof Error ? error.message : 'Erro desconhecido'}`));
    }
  });
};

// Transcribe audio using GROQ Whisper API with automatic handling of large files
export const transcribeAudioWithGroq = async (
  audioFile: File
): Promise<TranscriptionResult> => {
  try {
    const settings = getGroqSettings();
    
    if (!settings.groqApiKey) {
      console.error('No GROQ API key configured. Please add your API key in Settings.');
      throw new Error('API key not configured');
    }
    
    // Verificar e corrigir modelo Whisper se necess√°rio
    if (settings.whisperModel !== 'whisper-large-v3') {
      console.warn(`‚ö†Ô∏è Modelo Whisper incorreto detectado: ${settings.whisperModel}, corrigindo para whisper-large-v3`);
      settings.whisperModel = 'whisper-large-v3';
    }
    
    console.log(`üîß Configura√ß√µes de transcri√ß√£o:`, {
      whisperModel: settings.whisperModel,
      language: settings.language,
      apiKey: settings.groqApiKey ? 'Configurado' : 'N√£o configurado'
    });

    // Verificar tamanho do arquivo
    const fileInfo = getAudioFileInfo(audioFile);
    console.log(`üìÅ Arquivo de √°udio: ${fileInfo.sizeInMB}MB (${fileInfo.isLarge ? 'Grande' : 'Normal'})`);

    let fileToProcess = audioFile;
    let needsChunking = false;

    // Se o arquivo √© muito grande, comprimir primeiro
    if (fileInfo.needsCompression) {
      console.log('üîÑ Comprimindo arquivo de √°udio...');
      try {
        fileToProcess = await compressAudioFile(audioFile, 5); // Comprimir para 5MB (mais agressivo)
        console.log('‚úÖ Compress√£o conclu√≠da');
      } catch (error) {
        console.warn('‚ö†Ô∏è Falha na compress√£o, tentando com arquivo original');
      }
    }

    // Se ainda √© muito grande ap√≥s compress√£o, dividir em chunks
    if (getAudioFileInfo(fileToProcess).needsChunking) {
      console.log('üîÑ Arquivo muito grande, dividindo em chunks...');
      needsChunking = true;
      const chunks = await splitAudioIntoChunks(fileToProcess);
      
      // Processar cada chunk
      const chunkResults: TranscriptionResult[] = [];
      for (let i = 0; i < chunks.length; i++) {
        console.log(`üìù Processando chunk ${i + 1}/${chunks.length}...`);
        try {
          const chunkResult = await transcribeAudioChunk(chunks[i], settings);
          chunkResults.push(chunkResult);
        } catch (error) {
          console.warn(`‚ö†Ô∏è Chunk ${i + 1} falhou, tentando compress√£o adicional...`);
          
          // Tentar comprimir o chunk individualmente
          try {
            const compressedChunk = await compressAudioFile(chunks[i], 3); // Comprimir para 3MB
            const retryResult = await transcribeAudioChunk(compressedChunk, settings);
            chunkResults.push(retryResult);
          } catch (retryError) {
            console.error(`‚ùå Chunk ${i + 1} falhou mesmo ap√≥s compress√£o:`, retryError);
            // Adicionar placeholder para manter a estrutura
            chunkResults.push({
              text: `[ERRO: Chunk ${i + 1} n√£o p√¥de ser processado - ${retryError instanceof Error ? retryError.message : 'Erro desconhecido'}]`,
              speakerSegments: []
            });
          }
        }
      }
      
      // Combinar resultados dos chunks
      const combinedText = chunkResults.map(result => result.text).join('\n\n--- CHUNK ---\n\n');
      return {
        text: combinedText,
        speakerSegments: [] // Speaker diarization n√£o dispon√≠vel para chunks
      };
    }

    // Processar arquivo √∫nico (comprimido ou original)
    return await transcribeAudioChunk(fileToProcess, settings);

  } catch (error) {
    console.error('‚ùå Erro na transcri√ß√£o de √°udio:', error);
    throw error;
  }
};

// Fun√ß√£o auxiliar para transcrever um chunk de √°udio
const transcribeAudioChunk = async (audioFile: File, settings: any): Promise<TranscriptionResult> => {
  console.log(`üéµ Transcrevendo chunk: ${audioFile.name} (${(audioFile.size / 1024 / 1024).toFixed(2)}MB)`);
  console.log(`üîß Modelo Whisper configurado: ${settings.whisperModel}`);
  
  // Create form data for file upload
  const formData = new FormData();
  formData.append('file', audioFile);
  formData.append('model', settings.whisperModel);
  formData.append('language', settings.language);
  formData.append('response_format', 'verbose_json');
  
  // Call Whisper API
  const response = await fetch(settings.whisperApiEndpoint, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${settings.groqApiKey}`
    },
    body: formData
  });

  if (!response.ok) {
    const errorText = await response.text();
    console.error('GROQ Whisper API error response:', errorText);
    let errorData;
    try {
      errorData = JSON.parse(errorText);
    } catch (e) {
      errorData = { error: { message: errorText } };
    }
    throw new Error(`Whisper API error: ${errorData.error?.message || response.statusText}`);
  }

  const data = await response.json();
  console.log('Whisper API response:', data);
  
  // Extract transcription
  const transcription = data.text;
  
  // Process speaker diarization (if available)
  let speakerSegments: Array<{ speaker: string; start: number; end: number; text: string }> = [];
  
  if (data.segments && Array.isArray(data.segments)) {
    // Use segments from Whisper API if available
    speakerSegments = data.segments.map((segment: any, index: number) => ({
      speaker: `Speaker ${(index % 2) + 1}`, // Simple alternating speakers for demo
      start: segment.start,
      end: segment.end,
      text: segment.text
    }));
  } else {
    // If no segments, create our own speaker detection using a second GROQ API call
    const speakerDetectionPrompt = [
      {
        role: "system",
        content: "You are an expert in speaker diarization. Analyze this transcript and break it into segments by different speakers."
      },
      {
        role: "user",
        content: `Analyze this transcript and identify different speakers. Return your analysis as a JSON array of objects with speaker, start (in seconds), end (in seconds), and text fields: ${transcription}`
      }
    ];
    
    const speakerAnalysis = await makeGroqAIRequest(speakerDetectionPrompt, 2048, 'meta-llama/llama-4-scout-17b-16e-instruct');
    
    try {
      // Try to parse the JSON response, handling potential Markdown code blocks
      const jsonMatch = speakerAnalysis.match(/```json\n([\s\S]*?)\n```/) || 
                      speakerAnalysis.match(/```\n([\s\S]*?)\n```/) ||
                      speakerAnalysis.match(/(\[[\s\S]*\])/);
                      
      const jsonString = jsonMatch ? jsonMatch[1] : speakerAnalysis;
      const parsedSegments = JSON.parse(jsonString);
      
      if (Array.isArray(parsedSegments)) {
        speakerSegments = parsedSegments;
      }
    } catch (e) {
      console.error('Error parsing speaker segments:', e);
      // Fallback to simple speaker split
      speakerSegments = [
        {
          speaker: "Speaker 1",
          start: 0,
          end: 60,
          text: transcription
        }
      ];
    }
  }
  
  return {
    text: transcription,
    speakerSegments
  };
};

// Interface for image analysis response
export interface ImageAnalysisResult {
  ocrText: string;
  faces: {
    id: number;
    confidence: number;
    region: { x: number; y: number; width: number; height: number };
  }[];
  licensePlates: string[];
  enhancementTechnique: string;
  confidenceScores?: {
    plate: string;
    scores: number[];
  };
}



// Interface for image enhancement response
export interface ImageEnhancementResult {
  enhancedImageUrl: string;
  enhancementTechnique: string;
}

// Enhance image with AI
export const enhanceImageWithGroq = async (
  imageUrl: string
): Promise<ImageEnhancementResult> => {
  try {
    const settings = getGroqSettings();
    
    if (!settings.groqApiKey) {
      console.error('No GROQ API key configured. Please add your API key in Settings.');
      throw new Error('API key not configured');
    }

    console.log('Enhancing image with GROQ Vision API...');
    
    const messages = [
      {
        role: "system",
        content: "Voc√™ √© um especialista em melhoria de imagens forenses. Analise a qualidade da imagem e descreva as t√©cnicas de melhoria que seriam aplicadas para an√°lise forense."
      },
      {
        role: "user", 
        content: `Analise esta imagem em base64 e descreva as t√©cnicas de melhoria que aplicaria para an√°lise forense (contraste, nitidez, corre√ß√£o de cor, etc.): ${imageUrl}`
      }
    ];
    
    const enhancementDescription = await makeGroqAIRequest(messages, 1024, 'meta-llama/llama-4-scout-17b-16e-instruct');

    console.log('Image enhancement analysis completed');
    
    // Since GROQ doesn't actually enhance images, we return the original with description
    // In a real implementation, this would integrate with image processing libraries
    return {
      enhancedImageUrl: imageUrl,
      enhancementTechnique: enhancementDescription.replace(/^(#|##|\*\*) .*\n/, '') || 
        'T√©cnicas de melhoria aplicadas: ajuste de contraste, nitidez e corre√ß√£o de ilumina√ß√£o para otimizar a an√°lise forense.'
    };
  } catch (error) {
    console.error('Error enhancing image:', error);
    throw error;
  }
};

// Interface para an√°lise de imagem em lote
export interface BatchImageAnalysisResult {
  totalImages: number;
  processedImages: number;
  failedImages: number;
  results: Array<{
    fileName: string;
    fileSize: number;
    analysisType: string;
    result: string;
    timestamp: string;
    success: boolean;
    error?: string;
  }>;
  summary: {
    totalFaces: number;
    totalPlates: number;
    totalTexts: number;
    commonObjects: string[];
    insights: string[];
  };
}

// Fun√ß√£o para an√°lise em lote de m√∫ltiplas imagens
export async function analyzeMultipleImages(
  images: File[],
  analysisType: 'comprehensive' | 'ocr' | 'face-detection' | 'plate-detection' = 'comprehensive'
): Promise<BatchImageAnalysisResult> {
  try {
    console.log(`üîç Iniciando an√°lise em lote de ${images.length} imagens`);
    
    const results: BatchImageAnalysisResult['results'] = [];
    let totalFaces = 0;
    let totalPlates = 0;
    let totalTexts = 0;
    const commonObjects: string[] = [];
    const insights: string[] = [];
    
    // Processar cada imagem
    for (let i = 0; i < images.length; i++) {
      const image = images[i];
      console.log(`üì∏ Processando imagem ${i + 1}/${images.length}: ${image.name}`);
      
      try {
        // Converter imagem para base64
        const base64Image = await convertImageToBase64(image);
        
                 // Analisar imagem com modelos de vis√£o computacional
         const analysisResult = await analyzeImageWithVisionModels(base64Image, analysisType);
        
        // Extrair informa√ß√µes para o resumo
        const extractedInfo = extractAnalysisInfo(analysisResult);
        totalFaces += extractedInfo.faces;
        totalPlates += extractedInfo.plates;
        totalTexts += extractedInfo.texts;
        commonObjects.push(...extractedInfo.objects);
        insights.push(...extractedInfo.insights);
        
        results.push({
          fileName: image.name,
          fileSize: image.size,
          analysisType,
          result: analysisResult,
          timestamp: new Date().toISOString(),
          success: true
        });
        
        console.log(`‚úÖ Imagem ${image.name} processada com sucesso`);
        
      } catch (error) {
        console.error(`‚ùå Erro ao processar imagem ${image.name}:`, error);
        
        results.push({
          fileName: image.name,
          fileSize: image.size,
          analysisType,
          result: '',
          timestamp: new Date().toISOString(),
          success: false,
          error: error instanceof Error ? error.message : 'Erro desconhecido'
        });
      }
    }
    
    // Gerar resumo consolidado
    const summary = {
      totalFaces,
      totalPlates,
      totalTexts,
      commonObjects: [...new Set(commonObjects)], // Remover duplicatas
      insights: [...new Set(insights)] // Remover duplicatas
    };
    
    const batchResult: BatchImageAnalysisResult = {
      totalImages: images.length,
      processedImages: results.filter(r => r.success).length,
      failedImages: results.filter(r => !r.success).length,
      results,
      summary
    };
    
    console.log(`‚úÖ An√°lise em lote conclu√≠da: ${batchResult.processedImages}/${batchResult.totalImages} imagens processadas`);
    return batchResult;
    
  } catch (error) {
    console.error('‚ùå Erro na an√°lise em lote:', error);
    throw error;
  }
}

// Fun√ß√£o para converter imagem para base64
async function convertImageToBase64(file: File): Promise<string> {
  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.onload = () => {
      const result = reader.result as string;
      const base64 = result.split(',')[1];
      resolve(base64);
    };
    reader.onerror = reject;
    reader.readAsDataURL(file);
  });
}

// Fun√ß√£o para extrair informa√ß√µes da an√°lise
function extractAnalysisInfo(analysisResult: string): {
  faces: number;
  plates: number;
  texts: number;
  objects: string[];
  insights: string[];
} {
  const lines = analysisResult.split('\n');
  let faces = 0;
  let plates = 0;
  let texts = 0;
  const objects: string[] = [];
  const insights: string[] = [];
  
  lines.forEach(line => {
    const trimmedLine = line.trim();
    
    if (trimmedLine.includes('Faces Detectadas:')) {
      const match = trimmedLine.match(/(\d+)/);
      if (match) faces = parseInt(match[1]);
    }
    
    if (trimmedLine.includes('Placas Detectadas:')) {
      const match = trimmedLine.match(/(\d+)/);
      if (match) plates = parseInt(match[1]);
    }
    
    if (trimmedLine.includes('Texto Identificado:')) {
      texts = 1; // Marcar que h√° texto
    }
    
    if (trimmedLine.includes('Objetos Identificados:')) {
      const match = trimmedLine.match(/- (.+)/);
      if (match) objects.push(match[1]);
    }
    
    if (trimmedLine.includes('An√°lise Investigativa:')) {
      const match = trimmedLine.match(/- (.+)/);
      if (match) insights.push(match[1]);
    }
  });
  
  return { faces, plates, texts, objects, insights };
}

// Fun√ß√£o para gerar relat√≥rio consolidado de an√°lise em lote
export async function generateBatchAnalysisReport(
  batchResult: BatchImageAnalysisResult,
  caseContext?: string
): Promise<string> {
  try {
    const prompt = `Gere um relat√≥rio consolidado de investiga√ß√£o criminal baseado na an√°lise de ${batchResult.totalImages} imagens.

**DADOS DA AN√ÅLISE:**
- Total de imagens: ${batchResult.totalImages}
- Imagens processadas com sucesso: ${batchResult.processedImages}
- Imagens com falha: ${batchResult.failedImages}

**RESUMO ESTAT√çSTICO:**
- Total de faces detectadas: ${batchResult.summary.totalFaces}
- Total de placas de ve√≠culos: ${batchResult.summary.totalPlates}
- Total de textos identificados: ${batchResult.summary.totalTexts}
- Objetos comuns encontrados: ${batchResult.summary.commonObjects.join(', ')}

**CONTEXTO DO CASO:**
${caseContext || 'An√°lise de evid√™ncias visuais para investiga√ß√£o criminal'}

**REQUISITOS DO RELAT√ìRIO:**
1. **Resumo Executivo**: Vis√£o geral dos achados principais
2. **An√°lise por Categoria**: Organizar achados por tipo (faces, placas, textos, objetos)
3. **Padr√µes Identificados**: Conectar informa√ß√µes entre as imagens
4. **Recomenda√ß√µes Investigativas**: Pr√≥ximos passos sugeridos
5. **Classifica√ß√£o de Evid√™ncias**: Priorizar achados por relev√¢ncia
6. **An√°lise Temporal**: Identificar sequ√™ncias ou cronologia se aplic√°vel

**FORMATO:**
- Use linguagem t√©cnica e profissional
- Estruture com t√≠tulos e subt√≠tulos claros
- Inclua tabelas resumo quando apropriado
- Destaque descobertas cr√≠ticas
- Forne√ßa recomenda√ß√µes acion√°veis

Responda em portugu√™s brasileiro de forma estruturada e profissional.`;

    const messages = [
      {
        role: 'system',
        content: 'Voc√™ √© um investigador criminal s√™nior especializado em an√°lise de evid√™ncias visuais e gera√ß√£o de relat√≥rios t√©cnicos.'
      },
      {
        role: 'user',
        content: prompt
      }
    ];

    return await makeGroqAIRequest(messages, 4096, 'meta-llama/llama-4-scout-17b-16e-instruct');
    
  } catch (error) {
    console.error('‚ùå Erro ao gerar relat√≥rio consolidado:', error);
    throw error;
  }
}

// Default export for the service
export default {
  getGroqSettings,
  saveGroqSettings,
  hasValidApiKey,
  makeGroqAIRequest,
  generateInvestigationReportWithGroq,
  processLinkAnalysisDataWithGroq,
  transcribeAudioWithGroq,
  analyzeImageWithGroq,
  enhanceImageWithGroq,
  getAvailableModels,
  getCurrentModelConfig,
  analyzeMultipleImages,
  generateBatchAnalysisReport,
  // Novas fun√ß√µes para √°udio
  isAudioFileTooLarge,
  getAudioFileInfo,
  compressAudioFile,
  splitAudioIntoChunks
};
